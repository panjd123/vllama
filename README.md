# vllama

ç±» Ollama çš„ vLLM ç®¡ç†å·¥å…·ï¼Œæ”¯æŒåŠ¨æ€æ¨¡å‹åŠ è½½ã€å¸è½½å’Œè‡ªåŠ¨æ·˜æ±°ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **è‡ªåŠ¨å¯åŠ¨** - API è¯·æ±‚æ—¶è‡ªåŠ¨å¯åŠ¨/å”¤é†’æ¨¡å‹
- **LRU è‡ªåŠ¨æ·˜æ±°** - æ˜¾å­˜ä¸è¶³æ—¶è‡ªåŠ¨ä¼‘çœ æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹
- **ä¸‰çº§ä¼‘çœ æ¨¡å¼** - L1(è½»åº¦)/L2(æ·±åº¦)/L3(å®Œå…¨åœæ­¢)
- **è¿›ç¨‹ç»„ç®¡ç†** - æœåŠ¡å™¨å…³é—­æ—¶è‡ªåŠ¨æ¸…ç†æ‰€æœ‰å­è¿›ç¨‹
- **å†…å­˜å­˜å‚¨** - æ‰€æœ‰çŠ¶æ€å­˜äºå†…å­˜ï¼Œé‡å¯åæ¸…ç©º
- **OpenAI å…¼å®¹** - æ ‡å‡† OpenAI API æ¥å£
- **å¤š GPU æ”¯æŒ** - çµæ´»é…ç½® GPU è®¾å¤‡å’Œå†…å­˜

## ğŸ“¦ å®‰è£…

### æ–¹å¼ä¸€ï¼šæœ¬åœ°å®‰è£…

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/panjd123/vllama.git
cd vllama

# ä½¿ç”¨ uv å®‰è£…ï¼ˆæ¨èï¼‰
uv sync

# æˆ–ä½¿ç”¨ pip
pip install -e .
```

### æ–¹å¼äºŒï¼šDocker Composeï¼ˆæ¨èï¼‰

#### å‰ç½®è¦æ±‚

- Docker >= 20.10
- Docker Compose >= 1.29
- NVIDIA GPU å’Œ [nvidia-docker](https://github.com/NVIDIA/nvidia-docker)
- å·²ä¸‹è½½çš„æ¨¡å‹ï¼ˆä½äº `~/.cache/huggingface`ï¼‰

#### å¿«é€Ÿå¯åŠ¨

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/panjd123/vllama.git
cd vllama

# æ„å»ºå¹¶å¯åŠ¨
docker compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker compose logs -f

# åœæ­¢æœåŠ¡
docker compose down
```

#### é…ç½®è¯´æ˜

æ‰€æœ‰é…ç½®é€šè¿‡ `docker-compose.yml` ä¸­çš„ç¯å¢ƒå˜é‡è®¾ç½®ï¼š

```yaml
environment:
  - VLLAMA_HOST=0.0.0.0           # ç›‘å¬åœ°å€
  - VLLAMA_PORT=33258             # æœåŠ¡ç«¯å£
  - VLLAMA_VLLM_PORT_START=33300  # vLLM å®ä¾‹èµ·å§‹ç«¯å£
  - VLLAMA_VLLM_PORT_END=34300    # vLLM å®ä¾‹ç»“æŸç«¯å£
  - VLLAMA_UNLOAD_TIMEOUT=1800    # è‡ªåŠ¨å¸è½½è¶…æ—¶ï¼ˆç§’ï¼‰
  - VLLAMA_UNLOAD_MODE=2          # å¸è½½æ¨¡å¼ (1/2/3)
  - HF_HOME=/root/.cache/huggingface  # HF ç¼“å­˜ç›®å½•
```

#### å·æŒ‚è½½

```yaml
volumes:
  # æŒ‚è½½ä¸»æœºçš„ Hugging Face ç¼“å­˜ï¼ˆç›´æ¥ä½¿ç”¨å·²ä¸‹è½½çš„æ¨¡å‹ï¼‰
  - ${HOME}/.cache/huggingface:/root/.cache/huggingface

  # vllama é…ç½®ç›®å½•
  - ./vllama_config:/root/.vllama
```

#### Docker ä¸­ä½¿ç”¨ CLI

```bash
# æŸ¥çœ‹è¿è¡Œä¸­çš„å®ä¾‹
docker compose exec vllama vllama ps

# å¯åŠ¨æ¨¡å‹
docker compose exec vllama vllama start Qwen/Qwen3-0.6B

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
docker compose exec vllama vllama list

# ä¸‹è½½æ¨¡å‹
docker compose exec vllama vllama pull BAAI/bge-m3
```

#### ç›´æ¥æ‹‰å–é•œåƒ

```bash
# ä» Docker Hub æ‹‰å–
docker pull panjd123/vllama:latest

# ä½¿ç”¨ docker-compose.ymlï¼ˆä¿®æ”¹ build ä¸º imageï¼‰
# image: panjd123/vllama:latest
docker compose up -d
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å¯åŠ¨æœåŠ¡å™¨

```bash
vllama serve --port 33258
```

### ä½¿ç”¨ APIï¼ˆè‡ªåŠ¨å¯åŠ¨æ¨¡å‹ï¼‰

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:33258/v1",
    api_key="not-needed"
)

# ç›´æ¥ä½¿ç”¨æ¨¡å‹ï¼Œvllama ä¼šè‡ªåŠ¨å¯åŠ¨
response = client.chat.completions.create(
    model="Qwen/Qwen3-0.6B",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

### ç®¡ç†æ¨¡å‹

```bash
# æŸ¥çœ‹æœåŠ¡å™¨ä¿¡æ¯
vllama info

# æŸ¥çœ‹æ‰€æœ‰æ¨¡å‹å®ä¾‹
vllama ps

# æ‰‹åŠ¨å¯åŠ¨æ¨¡å‹
vllama start "Qwen/Qwen3-0.6B"

# æ‰‹åŠ¨åœæ­¢æ¨¡å‹
vllama stop "Qwen/Qwen3-0.6B"

# ä¼‘çœ æ¨¡å‹
vllama sleep "Qwen/Qwen3-0.6B" --level 2

# é…ç½®æ¨¡å‹
vllama assign "Qwen/Qwen3-0.6B" --gpu-memory 0.85 --devices 0
```

## ğŸ¯ è‡ªåŠ¨æ·˜æ±°æœºåˆ¶

å½“æ˜¾å­˜ä¸è¶³æ—¶ï¼Œvllama ä¼šè‡ªåŠ¨ï¼š
1. æŒ‰ LRUï¼ˆæœ€è¿‘æœ€å°‘ä½¿ç”¨ï¼‰ç­–ç•¥é€‰æ‹©è¦æ·˜æ±°çš„æ¨¡å‹
2. é€ä¸ªå°†è¿è¡Œä¸­çš„æ¨¡å‹ä¼‘çœ ï¼ˆLevel 2ï¼‰
3. é‡Šæ”¾æ˜¾å­˜åå¯åŠ¨æ–°è¯·æ±‚çš„æ¨¡å‹

**ç¤ºä¾‹åœºæ™¯**ï¼š
```
8GB æ˜¾å­˜ï¼Œæ¯ä¸ªæ¨¡å‹éœ€è¦ 4GB

è¯·æ±‚ Model A â†’ å¯åŠ¨ (ä½¿ç”¨ 4GB)
è¯·æ±‚ Model B â†’ å¯åŠ¨ (ä½¿ç”¨ 8GBï¼Œæ˜¾å­˜æ»¡)
è¯·æ±‚ Model C â†’ è‡ªåŠ¨æ·˜æ±° Model A â†’ å¯åŠ¨ Model C

æœ€ç»ˆ: Model A (sleeping), Model B (running), Model C (running)
```

## âš™ï¸ é…ç½®

### ç¯å¢ƒå˜é‡

```bash
export HF_HOME=/path/to/huggingface  # Hugging Face ä¸»ç›®å½•ï¼ˆæ¨¡å‹ç¼“å­˜ä½äº $HF_HOME/hubï¼‰
```

### æ¨¡å‹é…ç½®æ–‡ä»¶

`~/.vllama/models.yaml`:

```yaml
Qwen/Qwen3-0.6B:
  gpu_memory_utilization: 0.85
  max_model_len: 32768
  devices: [0]
  tensor_parallel_size: 1
```

### ä¸‰çº§ä¼‘çœ æ¨¡å¼

| çº§åˆ« | é‡Šæ”¾å†…å­˜ | æ¢å¤é€Ÿåº¦ | ç”¨é€” |
|-----|---------|---------|-----|
| **L1** | éƒ¨åˆ† KV cache | æœ€å¿« (ç§’çº§) | çŸ­æœŸç©ºé—² |
| **L2** | Weights + KV cache | ä¸­ç­‰ (10-30ç§’) | è‡ªåŠ¨æ·˜æ±° |
| **L3** | å®Œå…¨åœæ­¢è¿›ç¨‹ | æœ€æ…¢ (éœ€é‡å¯) | é•¿æœŸä¸ç”¨ |

## ğŸ“ CLI å‘½ä»¤

### vllama serve
```bash
vllama serve [--host HOST] [--port PORT] [--log-level LEVEL]
```
å¯åŠ¨ vllama æœåŠ¡å™¨ï¼ˆé»˜è®¤: `0.0.0.0:33258`ï¼‰

### vllama info
```bash
vllama info
```
æŸ¥çœ‹æœåŠ¡å™¨ä¿¡æ¯ï¼ˆç«¯å£ã€é…ç½®ã€å¥åº·çŠ¶æ€ç­‰ï¼‰

### vllama ps
```bash
vllama ps
```
æŸ¥çœ‹æ‰€æœ‰æ¨¡å‹å®ä¾‹çŠ¶æ€å’Œæœ€åè®¿é—®æ—¶é—´

### vllama start / stop
```bash
vllama start MODEL    # å¯åŠ¨æˆ–å”¤é†’æ¨¡å‹
vllama stop MODEL     # åœæ­¢æ¨¡å‹ï¼ˆæ¸…ç† ERROR/STARTING çŠ¶æ€ï¼‰
```

### vllama sleep
```bash
vllama sleep MODEL [--level {1,2,3}]
```
ä¼‘çœ æ¨¡å‹ï¼ˆé»˜è®¤ Level 2ï¼‰

### vllama assign
```bash
vllama assign MODEL \
  [--devices DEVICES] \
  [--gpu-memory RATIO] \
  [--restart]
```
é…ç½®æ¨¡å‹å‚æ•°å¹¶å¯é€‰é‡å¯

## ğŸ”Œ API ç«¯ç‚¹

| ç«¯ç‚¹ | è¯´æ˜ |
|-----|------|
| `GET /health` | å¥åº·æ£€æŸ¥ |
| `GET /v1/models` | åˆ—å‡ºæ‰€æœ‰æ¨¡å‹ |
| `POST /v1/chat/completions` | èŠå¤©è¡¥å…¨ |
| `POST /v1/completions` | æ–‡æœ¬è¡¥å…¨ |
| `POST /v1/embeddings` | ç”ŸæˆåµŒå…¥ |
| `POST /v1/rerank` | é‡æ’åº |
| `POST /v1/score` | è¯„åˆ† |

## ğŸ›‘ åœæ­¢æœåŠ¡å™¨

### æ–¹æ³• 1: ä½¿ç”¨ PID
```bash
# æŸ¥æ‰¾è¿›ç¨‹
ps aux | grep "vllama.*serve"

# åœæ­¢
kill <PID>
```

### æ–¹æ³• 2: å¿«æ·æ–¹å¼
```bash
kill $(pgrep -f "vllama.*serve")
```

**é‡è¦**: åœæ­¢ vllama æœåŠ¡å™¨ä¼šï¼š
- âœ… è‡ªåŠ¨ç»ˆæ­¢æ‰€æœ‰ vLLM å­è¿›ç¨‹
- âœ… æ¸…ç©ºæ‰€æœ‰å†…å­˜ä¸­çš„çŠ¶æ€
- âœ… ä¼˜é›…å…³é—­ï¼ˆç­‰å¾…å½“å‰è¯·æ±‚å®Œæˆï¼‰

## ğŸ“š å¸¸è§é—®é¢˜

### Q: æ˜¾å­˜ä¸è¶³å¯¼è‡´æ¨¡å‹å¯åŠ¨å¤±è´¥ï¼Ÿ

**A**: vllama ä¼šè‡ªåŠ¨æ·˜æ±°æ—§æ¨¡å‹ã€‚å¦‚æœä»å¤±è´¥ï¼š
```bash
# é™ä½å†…å­˜ä½¿ç”¨ç‡
vllama assign "MODEL" --gpu-memory 0.7

# é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
# ç¼–è¾‘ ~/.vllama/models.yaml
MODEL:
  max_model_len: 16384
```

### Q: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹æ—¥å¿—ï¼Ÿ

**A**: æ—¥å¿—æ–‡ä»¶ä½äº `~/.vllama/logs/`
```bash
# å¯åŠ¨æ¨¡å‹æ—¶ä¼šæ˜¾ç¤ºæ—¥å¿—è·¯å¾„
vllama start "MODEL"
# Logs: /home/user/.vllama/logs/MODEL_33300.log

# å®æ—¶æŸ¥çœ‹
tail -f ~/.vllama/logs/MODEL_33300.log
```

### Q: é‡å¯ vllama åæ¨¡å‹çŠ¶æ€ä¸¢å¤±ï¼Ÿ

**A**: è¿™æ˜¯è®¾è®¡è¡Œä¸ºã€‚çŠ¶æ€å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œé‡å¯åè‡ªåŠ¨æ¸…ç©ºã€‚æ¨¡å‹ä¼šåœ¨ä¸‹æ¬¡ API è¯·æ±‚æ—¶è‡ªåŠ¨å¯åŠ¨ã€‚

### Q: å¦‚ä½•ä½¿ç”¨å¤š GPUï¼Ÿ

**A**: ä½¿ç”¨ `assign` å‘½ä»¤é…ç½®ï¼š
```bash
# æ¨¡å‹ A ä½¿ç”¨ GPU 0
vllama assign "ModelA" --devices 0

# æ¨¡å‹ B ä½¿ç”¨ GPU 1
vllama assign "ModelB" --devices 1

# æ¨¡å‹ C ä½¿ç”¨å¤šå¡å¹¶è¡Œ
vllama assign "ModelC" --devices 0,1
```

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
vllama/
â”œâ”€â”€ vllama/
â”‚   â”œâ”€â”€ cli.py           # CLI å‘½ä»¤
â”‚   â”œâ”€â”€ config.py        # é…ç½®ç®¡ç†
â”‚   â”œâ”€â”€ gpu.py           # GPU ç›‘æ§
â”‚   â”œâ”€â”€ instance.py      # å®ä¾‹ç®¡ç†ï¼ˆè‡ªåŠ¨æ·˜æ±°ï¼‰
â”‚   â”œâ”€â”€ models.py        # æ¨¡å‹ä¿¡æ¯
â”‚   â”œâ”€â”€ scheduler.py     # è‡ªåŠ¨å¸è½½è°ƒåº¦
â”‚   â”œâ”€â”€ server.py        # FastAPI æœåŠ¡å™¨
â”‚   â”œâ”€â”€ state.py         # å†…å­˜çŠ¶æ€ç®¡ç†
â”‚   â””â”€â”€ yaml_manager.py  # YAML é…ç½®
â”œâ”€â”€ tests/               # å•å…ƒæµ‹è¯•
â”œâ”€â”€ CLAUDE.md           # é¡¹ç›®æŒ‡ä»¤
â””â”€â”€ README.md           # æœ¬æ–‡æ¡£
```

## ğŸ§ª å¼€å‘

```bash
# è¿è¡Œæµ‹è¯•
pytest

# ä»£ç è¦†ç›–ç‡
pytest --cov=vllama

# ç‰¹å®šæµ‹è¯•
pytest tests/test_vllama.py::TestConfig
```

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ™ è‡´è°¢

åŸºäºä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®ï¼š
- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£ Web æ¡†æ¶
- [Typer](https://typer.tiangolo.com/) - CLI æ¡†æ¶
