# vllama

**åƒ Ollama ä¸€æ ·ç®€å•åœ°ä½¿ç”¨ vLLMï¼Œå¹¶ä¸”å…·æœ‰æå¿«çš„æ¨¡å‹åˆ‡æ¢é€Ÿåº¦** - æ— éœ€æ‰‹åŠ¨ç®¡ç†æ¨¡å‹ï¼Œè‡ªåŠ¨å¯åŠ¨ï¼Œå¼€ç®±å³ç”¨ï¼Œé«˜æ•ˆåˆ‡æ¢ï¼ˆåœ¨é¢„çƒ­åï¼Œæ¨¡å‹åˆ‡æ¢æ—¶é—´ä»…éœ€å‡ ç§’é’Ÿï¼ï¼‰

```bash
vllama serve

vllama ps/list/pull/start/stop

# ä½ å¯èƒ½éœ€è¦åœ¨ç¬¬ä¸€æ¬¡å¯åŠ¨æ—¶é…ç½®é’ˆå¯¹æŸä¸ªæ¨¡å‹çš„å¯åŠ¨å‚æ•°ï¼Œå¦åˆ™æ¯ä¸ªæ¨¡å‹ä¼šç”¨é»˜è®¤å‚æ•°å¯åŠ¨
vllama assign Qwen/Qwen3-30B-A3B-Instruct-2507 --devices 1 --gpu-memory-utilization 0.93 --max-model-len 32768 --trust-remote-code --extra enable-prefix-caching=true --restart
```

```python
import openai

client = openai.OpenAI(base_url="http://localhost:33258/v1", api_key="not-needed")

# ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€é¢„å…ˆåŠ è½½æ¨¡å‹
response = client.chat.completions.create(
    model="Qwen/Qwen3-0.6B",
    messages=[{"role": "user", "content": "Hello!"}]
)

# æ— æ„Ÿåˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹
response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[{"role": "user", "content": "Hello!"}]
)
```

## ğŸ’¡ ä¸ºä»€ä¹ˆé€‰æ‹© vllamaï¼Ÿ

| ä¼ ç»Ÿ vLLM | âœ¨ vllama |
|----------|----------|
| æ¯ä¸ªæ¨¡å‹éœ€è¦æ‰‹åŠ¨å¯åŠ¨ç‹¬ç«‹è¿›ç¨‹ | **è‡ªåŠ¨å¯åŠ¨** - API è¯·æ±‚æ—¶è‡ªåŠ¨åŠ è½½æ¨¡å‹ |
| éœ€è¦è®°ä½æ¯ä¸ªæ¨¡å‹çš„ç«¯å£ | **ç»Ÿä¸€å…¥å£** - æ‰€æœ‰æ¨¡å‹å…±ç”¨ä¸€ä¸ªç«¯ç‚¹ |
| æ˜¾å­˜ä¸è¶³æ—¶éœ€è¦æ‰‹åŠ¨åœæ­¢å…¶ä»–æ¨¡å‹ | **æ™ºèƒ½åˆ‡æ¢** - è‡ªåŠ¨æ·˜æ±°æœ€ä¹…æœªç”¨çš„æ¨¡å‹ |
| åˆ‡æ¢æ¨¡å‹éœ€è¦ç­‰å¾…æ¼«é•¿çš„ vLLM åˆå§‹åŒ– | **æ— æ„Ÿåˆ‡æ¢** - å€ŸåŠ© vLLM çš„ Sleep Mode åœ¨å‡ ç§’å†…è‡ªåŠ¨å®Œæˆåˆ‡æ¢ |

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ è‡ªåŠ¨å¯åŠ¨
æ— éœ€é¢„å…ˆå¯åŠ¨æ¨¡å‹ï¼ŒAPI è¯·æ±‚æ—¶è‡ªåŠ¨åŠ è½½ã€‚é¦–æ¬¡è¯·æ±‚ä¼šç­‰å¾…æ¨¡å‹åŠ è½½ï¼Œåç»­è¯·æ±‚ç›´æ¥ä½¿ç”¨ã€‚

### ğŸ”„ æ— æ„Ÿåˆ‡æ¢
æ˜¾å­˜ä¸è¶³æ—¶ï¼Œè‡ªåŠ¨ä¼‘çœ æœ€ä¹…æœªä½¿ç”¨çš„æ¨¡å‹ï¼ˆä¼‘çœ å’Œå”¤é†’çš„ä»£ä»·ä»…ä¸ºå‡ ç§’ï¼‰ï¼Œä¸ºæ–°æ¨¡å‹è…¾å‡ºç©ºé—´ã€‚æ•´ä¸ªè¿‡ç¨‹å®Œå…¨è‡ªåŠ¨ï¼Œæ— éœ€äººå·¥å¹²é¢„ã€‚

> è¯¥åŠŸèƒ½æ˜¯å€ŸåŠ© vLLM çš„ [Sleep Mode](https://docs.vllm.ai/en/latest/features/sleep_mode/) å®ç°çš„ï¼Œä½ å¯ä»¥æŸ¥çœ‹ vLLM çš„å®˜æ–¹ blog äº†è§£æ›´å¤šç»†èŠ‚ï¼š[Zero-Reload Model Switching with vLLM Sleep Mode](https://blog.vllm.ai/2025/10/26/sleep-mode.html)

### ğŸ§  æ™ºèƒ½ç®¡ç†
- **LRU æ·˜æ±°ç­–ç•¥** - ä¼˜å…ˆæ·˜æ±°æœ€å°‘ä½¿ç”¨çš„æ¨¡å‹
- **ä¸‰çº§ä¼‘çœ ** - ä»ç§’çº§æ¢å¤åˆ°å®Œå…¨åœæ­¢çš„çµæ´»ç­–ç•¥
- **è‡ªåŠ¨ä¼˜åŒ–** - æ™ºèƒ½è®¡ç®—æ˜¾å­˜åˆ©ç”¨ç‡å’Œå‚æ•°
- **å¤š GPU æ”¯æŒ** - è‡ªåŠ¨é€‰æ‹©æœ€å¤§æ˜¾å­˜çš„ GPU

### ğŸ”Œ å®Œå…¨å…¼å®¹
- **OpenAI API** - ç›´æ¥æ›¿æ¢ OpenAI ç«¯ç‚¹å³å¯ä½¿ç”¨
- **æµå¼è¾“å‡º** - æ”¯æŒ SSE æµå¼å“åº”
- **å¤šç§ä»»åŠ¡** - Chatã€Completionã€Embeddingã€Rerank å…¨æ”¯æŒ

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆDocker - æ¨èï¼‰

```bash
git clone https://github.com/panjd123/vllama.git && cd vllama

docker compose up -d

curl http://localhost:33258/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen/Qwen3-0.6B",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### ä½¿ç”¨é¢„æ„å»ºé•œåƒ

```bash
# ç›´æ¥æ‹‰å–é•œåƒ
docker pull panjd123/vllama:latest

docker compose up -d
```

### Docker é…ç½®

ä¸»æœºçš„æ¨¡å‹ç¼“å­˜ä¼šè‡ªåŠ¨æŒ‚è½½åˆ°å®¹å™¨ï¼Œæ— éœ€é‡å¤ä¸‹è½½ï¼š

```yaml
volumes:
  - ${HOME}/.cache/huggingface:/root/.cache/huggingface  # æ¨¡å‹è‡ªåŠ¨å…±äº«
  - ./vllama_config:/root/.vllama                        # é…ç½®æŒä¹…åŒ–
```

é€šè¿‡ç¯å¢ƒå˜é‡è‡ªå®šä¹‰ï¼š

```yaml
environment:
  - VLLAMA_PORT=33258                    # æœåŠ¡ç«¯å£
  - VLLAMA_DEFAULT_DEVICE=0              # é»˜è®¤ GPUï¼ˆå¯é€‰ï¼ŒæœªæŒ‡å®šåˆ™é€‰æ‹©æ€»æ˜¾å­˜æœ€å¤§çš„ GPUï¼‰
  - VLLAMA_UNLOAD_TIMEOUT=1800           # ç©ºé—²å¤šä¹…åè‡ªåŠ¨å¸è½½
  - HF_HOME=/root/.cache/huggingface     # æ¨¡å‹ç¼“å­˜ä½ç½®
```

## ğŸ¬ å®æˆ˜æ¼”ç¤º

### åœºæ™¯ï¼š8GB æ˜¾å­˜ï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸‰ä¸ªæ¨¡å‹

```python
import openai

client = openai.OpenAI(base_url="http://localhost:33258/v1", api_key="not-needed")

# 1ï¸âƒ£ è¯·æ±‚ Model Aï¼ˆè‡ªåŠ¨å¯åŠ¨ï¼Œä½¿ç”¨ 4GBï¼‰
response = client.chat.completions.create(
    model="Qwen/Qwen3-0.6B",
    messages=[{"role": "user", "content": "ä½ å¥½"}]
)

# 2ï¸âƒ£ è¯·æ±‚ Model Bï¼ˆè‡ªåŠ¨å¯åŠ¨ä½¿ç”¨ 8GBï¼Œæ˜¾å­˜æ»¡ï¼‰
response = client.chat.completions.create(
    model="BAAI/bge-m3",
    messages=[{"role": "user", "content": "Hello"}]
)

# 3ï¸âƒ£ è¯·æ±‚ Model C
#    ğŸ”„ è‡ªåŠ¨æ·˜æ±° Model Aï¼ˆæœ€ä¹…æœªç”¨ï¼‰
#    â³ ç­‰å¾…å‡ ç§’é‡Šæ”¾æ˜¾å­˜
#    ğŸš€ å¯åŠ¨ Model C
response = client.chat.completions.create(
    model="google/gemma-3-270m-it",
    messages=[{"role": "user", "content": "Hi"}]
)

# 4ï¸âƒ£ å†æ¬¡è¯·æ±‚ Model A
#    ğŸ”„ è‡ªåŠ¨å”¤é†’ Model Aï¼ˆå‡ ç§’å†…æ¢å¤ï¼‰
#    âœ¨ æ— éœ€é‡æ–°åŠ è½½ï¼Œå¿«é€Ÿæ¢å¤
response = client.chat.completions.create(
    model="Qwen/Qwen3-0.6B",
    messages=[{"role": "user", "content": "å†è§"}]
)
```

**æ•´ä¸ªè¿‡ç¨‹å®Œå…¨è‡ªåŠ¨ï¼Œæ— éœ€ä»»ä½•æ‰‹åŠ¨æ“ä½œï¼**

## ğŸ“¦ æœ¬åœ°å®‰è£…

```bash
git clone https://github.com/panjd123/vllama.git && cd vllama

uv sync

# pip install -e .

vllama serve
```

## ğŸ“š æ·±å…¥äº†è§£

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# æœåŠ¡å™¨é…ç½®
export VLLAMA_HOST=0.0.0.0                    # ç›‘å¬åœ°å€
export VLLAMA_PORT=33258                      # æœåŠ¡ç«¯å£
export VLLAMA_DEFAULT_DEVICE=0                # é»˜è®¤ GPU ID

# è‡ªåŠ¨å¸è½½é…ç½®
export VLLAMA_UNLOAD_TIMEOUT=1800             # ç©ºé—² 30 åˆ†é’Ÿåè‡ªåŠ¨å¸è½½
export VLLAMA_UNLOAD_MODE=2                   # å¸è½½çº§åˆ« (1/2/3)

# æ¨¡å‹ç¼“å­˜
export HF_HOME=/path/to/huggingface           # æ¨¡å‹å­˜å‚¨ä½ç½®
```

### æ¨¡å‹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `~/.vllama/models.yaml` ä¸ºç‰¹å®šæ¨¡å‹é…ç½®å‚æ•°ï¼š

```yaml
Qwen/Qwen3-0.6B:
  gpu_memory_utilization: 0.85    # GPU æ˜¾å­˜ä½¿ç”¨ç‡
  max_model_len: 32768            # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
  devices: [0]                    # ä½¿ç”¨çš„ GPU
  tensor_parallel_size: 1         # å¼ é‡å¹¶è¡Œå¤§å°
  dtype: auto                     # æ•°æ®ç±»å‹
  trust_remote_code: false        # æ˜¯å¦ä¿¡ä»»è¿œç¨‹ä»£ç 
  auto_start: true                # æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½
```

ä¿®æ”¹åé‡å¯æ¨¡å‹åº”ç”¨é…ç½®ï¼š
```bash
vllama restart Qwen/Qwen3-0.6B
```

## ğŸ”§ CLI å‘½ä»¤å‚è€ƒ

### æœåŠ¡å™¨ç®¡ç†

```bash
vllama serve              # å¯åŠ¨æœåŠ¡å™¨
vllama info               # æŸ¥çœ‹æœåŠ¡å™¨ä¿¡æ¯
```

### æ¨¡å‹ç®¡ç†

```bash
vllama list                   # åˆ—å‡ºå¯ç”¨æ¨¡å‹
vllama pull MODEL             # ä¸‹è½½æ¨¡å‹
vllama ps                     # æŸ¥çœ‹è¿è¡ŒçŠ¶æ€
vllama start MODEL            # å¯åŠ¨æ¨¡å‹
vllama stop MODEL             # åœæ­¢æ¨¡å‹
vllama restart MODEL          # é‡å¯æ¨¡å‹
vllama sleep MODEL [-l 2]     # ä¼‘çœ æ¨¡å‹
vllama wake-up MODEL          # å”¤é†’æ¨¡å‹ï¼ˆå’Œ vllama start ç›¸åŒï¼‰

# é¢„çƒ­æ¨¡å‹ - é¢„å…ˆåŠ è½½æ¨¡å‹ä»¥åŠ å¿«é¦–æ¬¡è®¿é—®é€Ÿåº¦
vllama warm-up MODEL1 MODEL2          # ç«‹å³é¢„çƒ­æŒ‡å®šæ¨¡å‹
vllama warm-up MODEL --save           # ä¿å­˜åˆ°é…ç½®ï¼ŒæœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨é¢„çƒ­
vllama warm-up --show                 # æŸ¥çœ‹è‡ªåŠ¨é¢„çƒ­åˆ—è¡¨
vllama warm-up --remove MODEL         # ä»è‡ªåŠ¨é¢„çƒ­åˆ—è¡¨ä¸­ç§»é™¤
vllama warm-up --clear                # æ¸…ç©ºè‡ªåŠ¨é¢„çƒ­åˆ—è¡¨

# äº¤äº’å¼èŠå¤©
vllama run MODEL                      # å¯åŠ¨äº¤äº’å¼èŠå¤©ä¼šè¯
vllama run MODEL --system "prompt"    # ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
```

### é…ç½®ç®¡ç†

```bash
vllama assign MODEL [OPTIONS]

é€‰é¡¹ï¼š
  --gpu-memory, -m FLOAT          GPU æ˜¾å­˜ä½¿ç”¨ç‡ (0.1-1.0)
  --devices, -d TEXT              GPU è®¾å¤‡ ID (ä¾‹å¦‚: "0,1")
  --max-model-len, -l INT         æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
  --tensor-parallel-size, -t INT  å¼ é‡å¹¶è¡Œå¤§å°
  --dtype TEXT                    æ•°æ®ç±»å‹ (auto/float16/bfloat16/float32)
  --trust-remote-code             å¯ç”¨ä¿¡ä»»è¿œç¨‹ä»£ç 
  --no-trust-remote-code          ç¦ç”¨ä¿¡ä»»è¿œç¨‹ä»£ç 
  --auto-start                    å¯ç”¨æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½
  --no-auto-start                 ç¦ç”¨æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½
  --extra-args, -e TEXT           é¢å¤–å‚æ•° (key=valueï¼Œå¯å¤šæ¬¡ä½¿ç”¨)
  --clear-extra-args              æ¸…ç©ºæ‰€æœ‰é¢å¤–å‚æ•°
  --restart, -r                   åº”ç”¨é…ç½®åé‡å¯æ¨¡å‹
  --show, -s                      æ˜¾ç¤ºå½“å‰é…ç½®
```

ç¤ºä¾‹ï¼š
```bash
# åŸºæœ¬é…ç½®
vllama assign MODEL --devices 1 --gpu-memory 0.85

# å¯ç”¨ trust remote code å’Œ auto-start
vllama assign MODEL --trust-remote-code --auto-start

# ç¦ç”¨ auto-start
vllama assign MODEL --no-auto-start

# é…ç½®å¹¶é‡å¯
vllama assign MODEL --max-model-len 32768 --restart
```

### æ¨¡å‹é¢„çƒ­ (Warm-up)

é¢„çƒ­åŠŸèƒ½å…è®¸åœ¨æœåŠ¡å™¨å¯åŠ¨æ—¶è‡ªåŠ¨åŠ è½½å¸¸ç”¨æ¨¡å‹ï¼Œé¿å…é¦–æ¬¡ API è¯·æ±‚æ—¶çš„ç­‰å¾…æ—¶é—´ã€‚

**ä½¿ç”¨åœºæ™¯ï¼š**
- é¿å…é¦–æ¬¡è¯·æ±‚çš„å†·å¯åŠ¨å»¶è¿Ÿ
- è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹

**é…ç½®ç¤ºä¾‹ï¼š**

```bash
vllama warm-up Qwen/Qwen3-0.6B BAAI/bge-m3 --save
# æˆ–
# vllama assign Qwen/Qwen3-0.6B --auto-start
# vllama assign BAAI/bge-m3 --auto-start
vllama serve
```

é…ç½®ä¿å­˜åœ¨ `~/.vllama/models.yaml` ä¸­ï¼ˆä½œä¸ºæ¨¡å‹é…ç½®çš„ `auto_start` å­—æ®µï¼‰ï¼š
```yaml
Qwen/Qwen3-0.6B:
  auto_start: true
  # ... å…¶ä»–é…ç½®

BAAI/bge-m3:
  auto_start: true
  # ... å…¶ä»–é…ç½®
```

### ä¸‰çº§ä¼‘çœ æ¨¡å¼

vllama ä½¿ç”¨ vLLM çš„ä¼‘çœ åŠŸèƒ½å®ç°å¿«é€Ÿåˆ‡æ¢ï¼š

| çº§åˆ« | é‡Šæ”¾å†…å­˜ | æ¢å¤æ—¶é—´ | é»˜è®¤ |
|-----|---------|---------|----------|
| **L1** | Weights + KV cacheï¼Œæƒé‡ä¼šå¤‡ä»½åˆ° CPU å†…å­˜ä¸­ | ç§’çº§ |  |
| **L2** | Weights + KV cacheï¼Œæƒé‡ä¸ä¼šå¤‡ä»½åˆ° CPU å†…å­˜ä¸­ | ç§’çº§ | é»˜è®¤é…ç½® |
| **L3** | å®Œå…¨åœæ­¢è¿›ç¨‹ | åˆ†é’Ÿçº§ | |

## ğŸŒ API ç«¯ç‚¹

vllama æä¾›å®Œæ•´çš„ OpenAI å…¼å®¹ APIï¼š

| ç«¯ç‚¹ | è¯´æ˜ | ç¤ºä¾‹æ¨¡å‹ |
|-----|------|---------|
| `POST /v1/chat/completions` | èŠå¤©è¡¥å…¨ | Qwen, Llama, Gemma |
| `POST /v1/completions` | æ–‡æœ¬è¡¥å…¨ | ä»»ä½•è¯­è¨€æ¨¡å‹ |
| `POST /v1/embeddings` | ç”ŸæˆåµŒå…¥ | bge-m3, e5 |
| `POST /v1/rerank` | é‡æ’åº | reranker æ¨¡å‹ |
| `POST /v1/score` | è¯„åˆ† | è¯„åˆ†æ¨¡å‹ |
| `GET /v1/models` | åˆ—å‡ºæ¨¡å‹ | - |
| `GET /health` | å¥åº·æ£€æŸ¥ | - |

### ä½¿ç”¨ç¤ºä¾‹

**Chat è¡¥å…¨ï¼ˆæµå¼ï¼‰**
```python
import openai

client = openai.OpenAI(base_url="http://localhost:33258/v1", api_key="not-needed")

stream = client.chat.completions.create(
    model="Qwen/Qwen3-0.6B",
    messages=[{"role": "user", "content": "è®²ä¸ªç¬‘è¯"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

**Embedding**
```python
response = client.embeddings.create(
    model="BAAI/bge-m3",
    input=["Hello world", "ä½ å¥½ä¸–ç•Œ"]
)

embeddings = [data.embedding for data in response.data]
```

**äº¤äº’å¼èŠå¤©ï¼ˆå‘½ä»¤è¡Œï¼‰**
```bash
# å¯åŠ¨äº¤äº’å¼èŠå¤©ä¼šè¯
vllama run Qwen/Qwen3-0.6B

# ä½¿ç”¨è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯
vllama run Qwen/Qwen3-0.6B --system "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„ AI åŠ©æ‰‹"

# é€€å‡ºæ–¹å¼ï¼šè¾“å…¥ /exit æˆ–æŒ‰ Ctrl+D æˆ– Ctrl+C
```

## â“ å¸¸è§é—®é¢˜

<details>
<summary><b>Q: å¦‚ä½•æŒ‡å®šé»˜è®¤ä½¿ç”¨å“ªå¼  GPUï¼Ÿ</b></summary>

é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®ï¼š
```bash
export VLLAMA_DEFAULT_DEVICE=1  # ä½¿ç”¨ GPU 1
```

æˆ–åœ¨ docker-compose.yml ä¸­ï¼š
```yaml
environment:
  - VLLAMA_DEFAULT_DEVICE=1
```

æœªè®¾ç½®æ—¶ï¼Œvllama ä¼šè‡ªåŠ¨é€‰æ‹©æ€»æ˜¾å­˜æœ€å¤§çš„ GPUã€‚
</details>

<details>
<summary><b>Q: å¦‚ä½•åœ¨å¤š GPU ç¯å¢ƒä¸‹è¿è¡Œä¸åŒæ¨¡å‹ï¼Ÿ</b></summary>

```bash
# Model A åœ¨ GPU 0
vllama assign ModelA --devices 0

# Model B åœ¨ GPU 1
vllama assign ModelB --devices 1

# Model C ä½¿ç”¨å¤šå¡å¹¶è¡Œï¼ˆGPU 0,1ï¼‰
vllama assign ModelC --devices 0,1 --tensor-parallel-size 2
```
</details>

<details>
<summary><b>Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ</b></summary>

vllama ä¼šè‡ªåŠ¨æ·˜æ±°æ—§æ¨¡å‹ï¼Œå¦‚æœä»ç„¶å¤±è´¥ï¼š

```bash
# 1. é™ä½æ˜¾å­˜ä½¿ç”¨ç‡
vllama assign MODEL --gpu-memory 0.7 --restart

# 2. é™åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
vllama assign MODEL --max-model-len 16384 --restart

# 3. æ‰‹åŠ¨é‡Šæ”¾æŸä¸ªæ¨¡å‹
vllama stop MODEL
```
</details>

<details>
<summary><b>Q: å¦‚ä½•æŸ¥çœ‹æ¨¡å‹åŠ è½½æ—¥å¿—ï¼Ÿ</b></summary>

æ—¥å¿—ä½äº `~/.vllama/logs/`ï¼š

```bash
# æŸ¥çœ‹ç‰¹å®šæ¨¡å‹çš„æ—¥å¿—
tail -f ~/.vllama/logs/Qwen_Qwen3-0.6B_33300.log

# Docker ä¸­
docker compose exec vllama tail -f /root/.vllama/logs/Qwen_Qwen3-0.6B_33300.log
```
</details>

<details>
<summary><b>Q: Docker å®¹å™¨å¦‚ä½•ä½¿ç”¨ä¸»æœºå·²ä¸‹è½½çš„æ¨¡å‹ï¼Ÿ</b></summary>

docker-compose.yml å·²è‡ªåŠ¨é…ç½®å·æŒ‚è½½ï¼š

```yaml
volumes:
  - ${HOME}/.cache/huggingface:/root/.cache/huggingface
```

å®¹å™¨ä¼šç›´æ¥ä½¿ç”¨ä¸»æœºçš„æ¨¡å‹ï¼Œæ— éœ€é‡å¤ä¸‹è½½ã€‚
</details>

## ğŸ¤ å¯¹æ¯” Ollama

| ç‰¹æ€§ | Ollama | vllama |
|-----|--------|--------|
| æ¨ç†åç«¯ | llama.cpp (CPU/GPU) | vLLM (GPU only, æ›´å¿«) |
| ä½¿ç”¨æ–¹å¼ | âœ… ä¸€é”®å¯åŠ¨ | âœ… ä¸€é”®å¯åŠ¨ |
| è‡ªåŠ¨åŠ è½½ | âœ… | âœ… |
| æ¨¡å‹åˆ‡æ¢ | âœ… è‡ªåŠ¨å¸è½½æ—§æ¨¡å‹ | âœ… LRU æ™ºèƒ½æ·˜æ±° |
| API å…¼å®¹ | âœ… OpenAI å…¼å®¹ | âœ… OpenAI å…¼å®¹ |
| å¿«é€Ÿåˆ‡æ¢ | âœ… ç§’çº§é‡æ–°åŠ è½½ | âœ… ç§’çº§å¿«é€Ÿå”¤é†’ |
| æµå¼è¾“å‡º | âœ… | âœ… |
| é€‚ç”¨åœºæ™¯ | è½»é‡éƒ¨ç½² | GPU æ¨ç†ã€é«˜æ€§èƒ½æ‰¹é‡è¯·æ±‚éœ€æ±‚ |

**vllama = Ollama çš„æ˜“ç”¨æ€§ + vLLM çš„é«˜æ€§èƒ½**

## ğŸ™ è‡´è°¢

vllama åŸºäºä»¥ä¸‹ä¼˜ç§€å¼€æºé¡¹ç›®æ„å»ºï¼š

- [vLLM](https://github.com/vllm-project/vllm) - é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“
- [FastAPI](https://fastapi.tiangolo.com/) - ç°ä»£ Python Web æ¡†æ¶
- [Typer](https://typer.tiangolo.com/) - ä¼˜é›…çš„ CLI æ¡†æ¶

## TODO

- æµ‹è¯•å¤šå¡ç¯å¢ƒï¼ˆç›®å‰ä»…æµ‹è¯•äº†å¤šå¡ä¸‹åˆ†åˆ«ç”¨å•å¡ï¼‰

## ğŸ“„ è®¸å¯è¯

MIT License - è‡ªç”±ä½¿ç”¨ã€ä¿®æ”¹å’Œåˆ†å‘
