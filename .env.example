# vllama Docker Compose 环境变量配置示例
# 复制此文件为 .env 并根据需要修改

# ==================== Offline Mode ====================
# 启用 offline 模式以缓存模型列表（生产环境推荐）
# true: 首次扫描后缓存模型列表，减少磁盘 I/O
# false: 每次请求都重新扫描（开发环境推荐）
VLLAMA_OFFLINE_MODE=true

# ==================== Server Configuration ====================
VLLAMA_HOST=0.0.0.0
VLLAMA_PORT=33258

# ==================== vLLM Instance Ports ====================
VLLAMA_VLLM_PORT_START=33300
VLLAMA_VLLM_PORT_END=33400

# ==================== Model Cache ====================
# HuggingFace 模型缓存目录
HF_HOME=/root/.cache/huggingface

# vllama 配置和日志目录
VLLAMA_CONFIG=/root/.vllama

# ==================== Auto-Unload Settings ====================
# 模型空闲多久后自动卸载（秒）
VLLAMA_UNLOAD_TIMEOUT=1800

# 卸载模式：1=浅度休眠, 2=深度休眠, 3=完全停止
VLLAMA_UNLOAD_MODE=2

# ==================== GPU Settings ====================
# 默认 GPU 设备 ID（留空则自动选择）
# VLLAMA_DEFAULT_DEVICE=0

# ==================== Local Paths (宿主机) ====================
# 宿主机上的 HuggingFace 缓存目录
# HF_HOME=~/.cache/huggingface

# 宿主机上的 vllama 配置目录
# VLLAMA_CONFIG=~/.vllama
